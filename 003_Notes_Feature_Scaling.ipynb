{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e4eb2cb-d19c-4707-8eb3-8b9a3d67d362",
   "metadata": {},
   "source": [
    "# Feature Scaling\n",
    "\n",
    "Feature scaling is a **data preprocessing technique** used to bring all numerical features (variables) into the same range or scale without distorting differences in the values.\n",
    "\n",
    "Or you can simply say scaling adjusts the **values of features** so that they **have a comparable range or distribution**, especially when features are on **very different scales**. This helps machine learning models treat each feature more fairly during training.\n",
    "\n",
    "Example:\n",
    "\n",
    "`Age` ranges from 20–70\n",
    "\n",
    "`Income` ranges from 20,000–100,000\n",
    "\n",
    "If we feed them directly into some models, the larger-scale feature (`Income`) will dominate over the smaller-scale one (`Age`).\n",
    "\n",
    "\n",
    "### Why Do We Use It?\n",
    "\n",
    "1. Prevents domination of large-scale features\n",
    "\n",
    "    * Models that rely on **distance** (KNN, K-means, PCA) or **gradient-based optimization** (Logistic/Linear Regression, Neural Networks) are heavily affected by the scale of features.\n",
    "\n",
    "    * Scaling ensures all features contribute **fairly**.\n",
    "\n",
    "2. Speeds up convergence\n",
    "\n",
    "    * For gradient descent–based models, scaled data helps the algorithm converge faster (because the cost function doesn’t zigzag).\n",
    "\n",
    "3. Improves interpretability\n",
    "\n",
    "    * In models like regression, the coefficients become easier to compare when features are on the same scale.\n",
    "\n",
    "\n",
    "### What If We Don’t Use It?\n",
    "\n",
    "* **KNN / K-means / PCA** → Distances get biased toward large-scale features.\n",
    "\n",
    "  Example: `Income` could dominate `Age`, even if `Age` is equally important.\n",
    "\n",
    "* **Gradient Descent–based models** → Training becomes slower, sometimes failing to converge properly.\n",
    "\n",
    "* **Regression models** → Coefficients become misleading, as features on larger scales appear artificially more important.\n",
    "\n",
    "* **Tree-based models (Decision Trees, Random Forest, XGBoost)** → They don’t need scaling because they split based on thresholds, not distances.\n",
    "\n",
    "\n",
    "### Types of Feature Scaling\n",
    "\n",
    "* **Min-Max Scaling (Normalization)** → Scales values to `[0,1]`\n",
    "$$x' = \\frac{x - x_{min}}{x_{max} - x_{min}}$$\n",
    "\n",
    "* **Standardization (Z-score Scaling)** → Scales values to mean = 0, std = 1\n",
    "$$x' = \\frac{x - \\mu}{\\sigma}$$\n",
    "\n",
    "* **Robust Scaler (IQR Scaling)** I → Uses median and IQR (good for outliers).\n",
    "\n",
    "    * Uses median and interquartile range, which makes it robust to outliers\n",
    "\n",
    "    * Formula:\n",
    "        $$x_{scaled} = \\frac{x - median}{IQR}$$ \n",
    "\n",
    "    * Use when: Your data has outliers.\n",
    "\n",
    "| Method          | Formula                                      | Use Case                                                     |\n",
    "| --------------- | -------------------------------------------- | ------------------------------------------------------------ |\n",
    "| Min-Max Scaling | $x' = \\frac{x - x_{min}}{x_{max} - x_{min}}$ | When you need data in a fixed \\[0,1] range                   |\n",
    "| Standardization | $x' = \\frac{x - \\mu}{\\sigma}$                | When you need mean=0, std=1 (works well with most ML models) |\n",
    "| Robust Scaler   | $x' = \\frac{x - median}{IQR}$                | When you have outliers                                       |\n",
    "\n",
    "**Min-Max scaling can be sensitive to outliers** — so *Standardization or Robust Scaler is preferred when extreme values exist*.\n",
    "\n",
    "And if in case you are wondering, if feature scaling makes you loose your data or it changes any pattern of your data then-NO, Feature scaling does NOT make you lose data, nor does it change the underlying pattern. \n",
    "\n",
    "### Feature Scaling Actually Does Not:\n",
    "\n",
    "* Drop any rows or columns\n",
    "\n",
    "* Remove information\n",
    "\n",
    "* Change relationships between points\n",
    "\n",
    "It’s like converting all distances from meters to kilometers — the numbers change, but the relative distances stay exactly the same."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b63c9e-2843-4620-9a1f-ce3057499414",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Effects on Outliers while using Different Scaling Technique\n",
    "\n",
    "1. Standardization (Z-score Scaling)\n",
    "\n",
    "    * Effect of Outliers:\n",
    "        * Outliers do affect the mean (μ) and standard deviation (σ), so the entire distribution shifts and stretches.\n",
    "\n",
    "    * What to Do:\n",
    "\n",
    "        * If outliers are legitimate data points (e.g., very high but real house prices), you usually keep them — you just need to be aware that scaling will reduce how far they look from the mean.\n",
    "\n",
    "        * If outliers are errors or noise, remove or cap them (e.g., using winsorization or z-score threshold filtering) before standardization.\n",
    "\n",
    "2. Robust Scaling\n",
    "\n",
    "    * Effect of Outliers:\n",
    "        * Almost no effect — because median and IQR are not influenced by extreme values.\n",
    "\n",
    "    * What to Do:\n",
    "\n",
    "        * You don’t need to remove outliers just to make robust scaling work — that’s exactly what robust scaling is designed for.\n",
    "\n",
    "        * You might still remove outliers if they are true anomalies that would distort your model, but not because of the scaling step.\n",
    "     \n",
    "---\n",
    "     \n",
    "#### Just to ignore all the effects you can just use a proper ML preprocessing pipeline.\n",
    "\n",
    "* Removing outliers first makes your scaling more meaningful, because mean/median/min/max are no longer distorted.\n",
    "\n",
    "* Scaling after outlier removal ensures all features are on a comparable scale, which helps most ML models train faster and perform better.\n",
    "\n",
    "* Training on the scaled data is completely valid and usually improves convergence (especially for models using gradient descent).\n",
    "\n",
    "\n",
    "### Why This Works So Well\n",
    "\n",
    "* **Statistical estimates become stable** → mean, std, min, max, IQR are all representative of your data.\n",
    "\n",
    "* **Scaling transforms are accurate** → no single extreme value dominates the transformation.\n",
    "\n",
    "* **Model learns faster** → weights update more evenly across features.\n",
    "\n",
    "* **No information loss (except bad outliers)** → your model still sees all relevant, legitimate data points.\n",
    "\n",
    "\n",
    "##### So Your Final Pipeline Looks Like:\n",
    "\n",
    "1. `Outlier Handling`\n",
    "\n",
    "    * Detect and remove extreme/unwanted outliers (or cap them if necessary).\n",
    "\n",
    "2. `Feature Scaling`\n",
    "\n",
    "    * Apply Min-Max / Standardization / Robust depending on your model and data distribution.\n",
    "\n",
    "3. `Model Training`\n",
    "\n",
    "    * Train your algorithm on the cleaned & scaled dataset.\n",
    "  \n",
    "---\n",
    "\n",
    "### When should you use IQR method or Z-Score to remove Outliers from you data:\n",
    "\n",
    "1. **IQR Method (Interquartile Range)**\n",
    "\n",
    "* How it works:\n",
    "\n",
    "    * Finds the 25th percentile (Q1) and 75th percentile (Q3).\n",
    "\n",
    "    * Computes IQR = Q3 - Q1.\n",
    "\n",
    "    * Anything below **Q1 - 1.5×IQR** or above **Q3 + 1.5×IQR** is considered an outlier.\n",
    "\n",
    "* Key Feature: Based on **median & percentiles** → very robust to extreme values.\n",
    "\n",
    "* When to Use:\n",
    "    * When your data is **not normally distributed** (skewed, long-tailed, etc.)\n",
    "    * When you want a **non-parametric** method (no assumption about data distribution)\n",
    "    * When you care about being robust to outliers themselves\n",
    "\n",
    "2. Z-Score Method\n",
    "\n",
    "* How it works:\n",
    "\n",
    "    * Calculates mean (μ) and standard deviation (σ).\n",
    "\n",
    "    * Computes Z = (X - μ) / σ.\n",
    "\n",
    "    * Anything with |Z| > 3 (commonly) is considered an outlier.\n",
    "\n",
    "* Key Feature: Based on **mean & standard deviation** → sensitive to extreme values.\n",
    "\n",
    "* When to Use:\n",
    "    * When your data is **approximately normal** (bell-shaped distribution)\n",
    "    * When you want to measure **how many standard deviations away** a point is\n",
    "    * When you prefer a probabilistic interpretation (Z > 3 ≈ rare event in normal distribution)\n",
    "\n",
    "### Big Difference\n",
    "\n",
    "* IQR works on ranks (position in sorted data) → robust, no assumption about shape\n",
    "\n",
    "* Z-Score works on distances from mean → assumes data is roughly symmetric/normal\n",
    "\n",
    "---\n",
    "\n",
    "### Rule of Thumb (Quick Decision)\n",
    "\n",
    "\n",
    "* Looks normal? → Use Z-Score\n",
    "\n",
    "* Looks skewed? → Use IQR\n",
    "\n",
    "* Not sure? → IQR is safer, since it doesn’t assume normality.\n",
    "\n",
    "\n",
    "| Situation                                | Best Choice |\n",
    "| ---------------------------------------- | ----------- |\n",
    "| Data is symmetric & bell-shaped          | **Z-score** |\n",
    "| Data is skewed or unknown distribution   | **IQR**     |\n",
    "| Small dataset (mean & std unreliable)    | **IQR**     |\n",
    "| Large dataset & want statistical meaning | **Z-score** |\n",
    "\n",
    "\n",
    "### Example\n",
    "\n",
    "Imagine this dataset:\n",
    "\n",
    "`[10, 12, 13, 15, 14, 11, 100]`\n",
    "\n",
    "* **Mean = 26.4, Std ≈ 31.3**\n",
    "\n",
    "    * Z-score for 100 = (100 - 26.4)/31.3 ≈ 2.34 → ❌ Not flagged if cutoff = 3\n",
    "\n",
    "* **IQR = Q3 - Q1 = 15 - 11.5 = 3.5**\n",
    "\n",
    "    * Upper bound = 15 + 1.5×3.5 = 20.25 → ✅ 100 is flagged as outlier\n",
    "\n",
    "So in this skewed dataset, IQR works better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04167f13-bc40-4e32-86ad-a68000828dc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
